Проще всего результат логистической регрессии представить в виде *сигмоиды*  
![[Pasted image 20221130214638.png|400]]  
Конечным результатом предсказания модели, обученной на логистической регрессии, будет вероятность отнесения к одному из двух исходов. Очень скомканный вывод формулы будет такой:  
Если $P(X)$ - вероятность события $X$, то отношение шансов будет $\frac{P(X)}{1-P(X)}$. Информативность одна, но в случае с $P(X)$ размерность $[0, 1]$, а в случае с отношением шансов - $[0, +\infty]$. Если его прологарифмировать, то вообще $[-\infty, +\infty]$. Преобразованиями и зависимостями конечная формула как раз получается сигмоида:  
$$\sigma(z)=\frac{1}{1+exp^{-z}}$$  
#### LOGIT преобразование  
Для ситуации, когда зависимая переменная является номинативной (2 исхода) можно сделать *logit преобразование*. 2 исхода мы можем поместить в вероятность результата одного исхода $[0; 1]$. Этого недостаточно, нужно достичь диапазона значений $[-∞;+∞]$.   
Для достижения диапазона $[0;+∞]$ можно получить *odds* (отношение вероятности успеха к вероятности неудачи) $\frac{0.8}{(1-0.8)}=4$.  
Для отрицательных значений можно всё логарифмировать в *logodds*. Тогда значение *odds* меньше 1 (p < 0.5) будет отрицательным. $log(\frac{0.2}{1-0.2})=-1.38$.  
$$logit(p)=log(\frac{p}{1-p})=β~_0~+β~_1~*x~_1~$$  
$$p=\frac{exp(β~_0~+β~_1~*x~_1~)}{1+exp(β~_0~+β~_1~*x~_1~)}$$  
### Предсказание  
Так как логистическая регрессия работает с вероятностями, обучение модели будет состоять только в том, чтобы все наблюдения поделить по группам (переменным) и каждой дать *веса* (logit преобразование).   
Итогом модели будет формула $β~_0~+β~_1~*x~_1~...$  
Каждый *вес* - это *состояние* переменной (например is male, is in 1st class)  
Важно заметить, что при обучении модели нужно настроить параметр *регуляризации* для контроля переобучения или недообучения модели  
![[Pasted image 20221208023708.png]]  
