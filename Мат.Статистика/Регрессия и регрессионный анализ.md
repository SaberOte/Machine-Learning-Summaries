#### Метод наименьших квадратов  
![[Pasted image 20220908182609.png]]  
  
На рис. синяя линия - это максимально приближенная линия, которая описывала бы направление и характер распределения данных. Метод наименьших квадратов подразумевает расчёт коэффициента b1, который позволит задать угол прямой. Впоследствии, это может помочь грязно экстраполировать данные.  
  
Коэффициент b0 нужен для того, чтобы понять, насколько среднее значение будет удалено от начала графика. Или же какое будет значение зависимой переменной, если все независимые будут равны нулю.  
  
![[Pasted image 20220908182622.png]]  
  
Если есть необходимость проверить гипотезу о том, что b1 = 0 (данные не имеют взаимосвязи), то используют t-критерий как на рис. выше (Т-тест. T-распределение помогает получить значение p-value).  
#### Коэффициент детерминации  
  
![[Pasted image 20220908182635.png]]  
  
Коэффициент детерминации - грубо говоря дисперсия вокруг прямой, которая характеризует корреляцию. Если он большой, значит значения очень близко лежат к прямой и данные ведут себя довольно предсказуемо. В обратном случае, если коэффициент равен нулю, то данные хаотичные и почти не взаимосвязаны. Поэтому он и называется квадратом коэффициента корреляции. Если R2 = 0,95, можно сказать, что 95 % изменчивости переменной объясняется нашей моделью.  
  
![[Pasted image 20220908182642.png]]  
  
Данные также следовало бы сначала проверить на несколько параметров. На рис. пример неправильных данных, но вполне нормальных коэффициентов корреляции.   
## **Требования к модели**:  
- Линейная зависимость переменных  
- Нормальное распределение остатков  
- Независимость остатков (например плохо если остатки по группам раскиданы)  
- Гетероскедастичность  
- Проверка на мультиколлинеарность (в случае с множественной регрессией)  
- Нормальное распределение переменных (желательно)  
Также есть множественная регрессия. Она позволяет проанализировать сразу несколько независимых переменных и их влияние на зависимую.  
![[Pasted image 20220908182701.png]]  
![](file:///C:/Users/Polka/AppData/Local/Temp/msohtmlclip1/01/clip_image006.gif)Анализ данных с рисунка: коэффициент выборки metro_res статистически значим (p = 0.006), можно сказать, что при неизменчивости других параметров, зависимая переменная будет расти на -0.06. Коэффициент детерминации (R2) как всегда помогает определить зависимость отклонения зав.переменной от независимых. Но в случае с множественной регрессией, нужно использовать исправленный коэффициент детерминации, потому что с ростом количества переменных растёт шанс найти случайные корреляции.  
  
#### Построение модели  
  
Также необходимо отсеять ненужные переменные, которые могут понижать коэффициент детерминации (R2).  
  
![](file:///C:/Users/Polka/AppData/Local/Temp/msohtmlclip1/01/clip_image008.gif)  
![[Pasted image 20220908182714.png]]  
На рис. видно, что переменная female_house очень сильно коррелирует со всеми другими переменными. Существует рофл, что, если предикторы сильно между собой коррелируют - это плохо. Если её убрать, то значение R2 будет лучше, поэтому от нее следует избавиться. (Это и называется **МУЛЬТИКОЛЛИНЕАРНОСТЬ**).  
  
Можно разбить на этапы построение итоговой регрессионной модели (отсебятина):  
-        Смотрим на графиках, подходят ли наши данные для регрессионного анализа  
-        Изучаем зависимости  
-        Убираем лишние переменные  
-        Выводим общую формулу, характеризующую регрессионную модель (рис. ниже)  
-        Смотрим опять же адекватность модели по графикам (например QQ Plot, Анализ остатков..)  
  
![[Pasted image 20220908182723.png]]  
![](file:///C:/Users/Polka/AppData/Local/Temp/msohtmlclip1/01/clip_image010.gif)  
  
Можно использовать _логистическую регрессию_, если у нас зависимая переменная номинативная (0 или 1).  
  
#### Выбор моделей  
Используется **критерий Акаике** для отбора наилучшей  
  
### Регрессии Лассо и Ridge  
Применение метода регрессии лассо или ridge необходимо для данных, в которых фичи очень сильно коррелируют, то есть имеется проблема **мультиколинеарности**. Обычная линейная регрессия напрямую подбирает наилучшие коэффициенты для точечного попадания в локальный минимум функции потерь по методы наименьших квадратов. Перечисленные регрессии пытаются минимизировать и уравновесить сразу два параметра - не только функцию потерь, но и штраф.   
![[Pasted image 20221209234114.png]]  
Регрессия Лассо представлена на рисунке слева. Форма ромба образуется за уравнения $|\beta_1| + |\beta_2| \leq t$, поэтому, если одна из фич не имеет весомого влияние на минимизацию функции потерь, за счёт штрафа она вообще может обнулиться и не участвовать в дальнейшем обучении и предсказании модели.   
Регрессия Ridge уравнением штрафа обрисовывает круг $\beta_1^2 + \beta^2_2 \geq 1$. Также хорошо борется с избыточностью данных, но у неё нет склонности так кардинально занулять фичи  
  
