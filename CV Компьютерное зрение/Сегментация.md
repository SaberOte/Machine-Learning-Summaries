## Метрики  
Все представленные метрики для сегментации лежат в диапазоне `[0,1]`, где 1 - идеальный коэффициент.  
#### Dice Coefficient  
$$2\frac {|X \cap Y|}{|X|+|Y|}$$  
, где X и Y - это множества пикселей (Predicted и Ground Truth для задач сегментации). |X| и |Y| - это площадь этих множеств. Таким образом, идеальным результатом будет, когда числитель будет равен половине знаменателя - то есть площадь пересечения будет в два раза меньше суммы площадей множеств, а иначе - пиксели будут в точности совпадать.  
Dice чаще всего используется в медицинских снимках.  
#### Jaccard Coefficient  
$$ \frac{|X\cap Y|}{|X\cup Y|} $$  
![Pasted image 20230607135859.png|200](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/CV%20Компьютерное%20зрение/attachments/Pasted%20image%2020230607135859.png?raw=true)  
Формула Жаккара оперирует не только площадями множеств, но и учитывает количество общих элементов множеств.  
#### Cosine Coefficient  
$$\frac {|X \cap Y|}{|X|^{\frac1 2}|Y|^{\frac1 2}}$$  
  
## Модели   
##### FCN8  
Самая простая реализация сегментации. У предобученной VGG или ResNet убираются Dense слои в конце (получается полно сверточная сеть **FCN**), а затем в конец добавляется слой Upsampling, чтобы размазать последний слой на исходную картинку.  
![Pasted image 20230607164743.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/CV%20Компьютерное%20зрение/attachments/Pasted%20image%2020230607164743.png?raw=true)  
Так как применяется самый грубый способ восстановления исходного разрешения изображения - Nearest Neighbor Upsampling, то границы получаются очень нечеткими и жёваными.   
Nearest Neighbor Upsampling - это увеличение разрешения изображения путём увеличения пикселя в два раза. То есть из одного пикселя 0xFF00FF получается матрица 2х2 таких пикселей.  
  
##### SegNet  
Следующий этап после FCN8. Здесь используется ступенчатая "развертка" модели. Помимо энкодера, здесь есть декодер, который немного плавнее и лучше размазывает фичи в изображение исходного разрешения.  
![Pasted image 20230607144223.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/CV%20Компьютерное%20зрение/attachments/Pasted%20image%2020230607144223.png?raw=true)  
  
##### UNet  
Мощный прорыв в области сегментации произошёл, когда пришла идея добавить в сеть skip connections. У каждого слоя декодера теперь есть входные данные такого же в иерархии слоя энкодера, что позволяет сделать 2 огромных улучшения: исчезает проблема затухающего градиента, за счет чего обучение проходит эффективнее, а также границы сегментированного изображения теперь получаются максимально плавными и четкими.  
![Pasted image 20230607145404.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/CV%20Компьютерное%20зрение/attachments/Pasted%20image%2020230607145404.png?raw=true)  
Может быть использована не только для задач сегментации объектов, но также и для задач регрессии (предсказание расстояния до объектов), оптических задач (направления векторов) и т.д.  
  
##### FPN (Feature Pyramid Networks)  
Для обычных сверточных сетей существует проблема масштаба. Если при тренировке модели объекты были, например, большого размера, а на вход поступает такой же объект но малого, то сеть будет вести себя непредсказуемо.  
  
Можно просто хорошо поработать на этапе аугментации, но придумали небольшую хитрость - делать предсказания, основываясь не только на последнем сверточном слое, но учитывать каждый слой сети.  
![Pasted image 20230607164222.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/CV%20Компьютерное%20зрение/attachments/Pasted%20image%2020230607164222.png?raw=true)  
Таким образом, допустим, человек крупным планом будет хорошо определяться на одном слое, а маленький силуэт на горизонте будет детектироваться на противоположном.  
На картинке представлено несколько подходов к реализации такой архитектуры.  
  
## Функции потерь  
В общих задачах почти всегда подходит стандартная **Binary / Categorical Cross Entropy**. НО!   
  
##### Soft Dice / Soft Jaccard  
Уравнение энтропии совсем не похоже на [[Сегментация#Dice Coefficient|Dice]] или [[Сегментация#Jaccard Coefficient|Jaccard]] коэффициенты.  
Коэффициенты же напрямую использовать для функции потерь тоже нельзя, так как они недифференцируемы, поэтому можно немного приспособить формулы, чтобы их можно было дифференцировать и брать градиент.   
Пример функции потерь, которая унаследована от Dice коэффициента и BCE (Binary Cross Entropy):  
$$LOSS = BCE - ln(DICE)$$  
$$BCE = -\sum_i{(y_iln(p_i)+(1-y_i)ln(1-p_i))}$$  
$$DICE=2\frac{\sum_i{y_ip_i}}{\sum y_i + \sum p_i}$$  
Видно, что DICE напоминает оригинальную формулу, где брались пересечения и сложения множеств.  
>По итогу, эксперименты показывают, что обучение сходиться чуть-чуть лучше.  
  
##### Lovasz-Softmax loss   
С этой функцией потерь можно ещё подтюнить обученную модель, чтобы немного дожать обучение.