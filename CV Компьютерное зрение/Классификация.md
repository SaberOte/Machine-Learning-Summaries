#### VGG'14 
Модель, которая в 2014 году уже добралась до ошибки в *6.8% top5 error* со *140кк параметрами*. Примечательна своей очень простой архитектурой - несколько **convolutional**/**maxpool** слоёв, затем кучка **полносвязных**. На картинке представлены несколько вариантов модели - **A**, **A-LRN**, **B**, **C**, **D**, **E**. При повышении количества слоёв плохо обучается, поэтому чаще используется ResNet.  
![[Pasted image 20230607001637.png|300]] 
#### ResNet'15
Если обучать очень мощные модели, по типу [[Классификация#VGG'14|VGG]], где очень много параметров и слоёв, то заметно, что обучение происходит очень плохо. Меньшее количество слоёв одной и той же модели может обучаться лучше при такой ситуации. Объяснение этому может быть следующее: изначально сеть заполнена шумными данными. При обучении градиент, конечно же, распространяется с конца в начало. Но в случае с очень мощными моделями, градиент затухает, не принося существенных изменений к началу сети.   
  
Чтобы с этим бороться, в 2015 году придумали сеть ResNet, в которой некоторые из слоёв не просто перемножают входные значения, а **модифицируют** входные значения.  
![[Pasted image 20230607003045.png]]  
Такое поведение **переносит** информацию с предыдущих слоёв, не давая затухать градиенту. Очень похоже на поведение *контекста* в модели [[RNN]].   
  
![[Pasted image 20230607004208.png]]  
Пример релизации может быть следующим:   
1. Взять VGG-19   
2. Добавить побольше слоёв  
3. Добавить **residual-connections**.   
Теперь сеть может обучаться крайне глубоко. Хоть на 1000 слоев - сеть все равно обучается.