Если обучать очень мощные модели, по типу [[Модели#^ac907a|VGG]], где очень много параметров и слоёв, то заметно, что обучение происходит очень плохо. Меньшее количество слоёв одной и той же модели может обучаться лучше при такой ситуации. Объяснение этому может быть следующее: изначально сеть заполнена шумными данными. При обучении градиент, конечно же, распространяется с конца в начало. Но в случае с очень мощными моделями, градиент затухает, не принося существенных изменений к началу сети.   
  
Чтобы с этим бороться, в 2015 году придумали сеть [[Модели#^94eaad|ResNet]], в которой некоторые из слоёв не просто перемножают входные значения, а **модифицируют** входные значения.  
![[Pasted image 20230607003045.png]]  
Такое поведение **переносит** информацию с предыдущих слоёв, не давая затухать градиенту. Очень похоже на поведение *контекста* в модели [[RNN]].   
  
Пример релизации может быть следующим:  
![[Pasted image 20230607004208.png]]  
1. Взять VGG-19   
2. Добавить побольше слоёв  
3. Добавить **residual-connections**.   
Теперь сеть может обучаться крайне глубоко. Хоть на 1000 слоев - сеть все равно обучается.