Если обучать очень мощные модели, по типу [[Модели#^ac907a|VGG]], где очень много параметров и слоёв, то заметно, что обучение происходит очень плохо. Меньшее количество слоёв одной и той же модели может обучаться лучше при такой ситуации. Объяснение этому может быть следующее: изначально сеть заполнена шумными данными. При обучении градиент, конечно же, распространяется с конца в начало. Но в случае с очень мощными моделями, градиент затухает, не принося существенных изменений к началу сети.   
  
Чтобы с этим бороться, в 2015 году придумали сеть [[Модели#^94eaad|ResNet]], в которой некоторые из слоёв не просто перемножают входные значения, а **модифицируют** входные значения.  
![Pasted image 20230607003045.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/CV%20Компьютерное%20зрение/attachments/Pasted%20image%2020230607003045.png?raw=true)  
Такое поведение **переносит** информацию с предыдущих слоёв, не давая затухать градиенту. Очень похоже на поведение *контекста* в модели [[RNN]].   
  
![Pasted image 20230607004208.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/CV%20Компьютерное%20зрение/attachments/Pasted%20image%2020230607004208.png?raw=true)  
Пример релизации может быть следующим:   
1. Взять VGG-19   
2. Добавить побольше слоёв  
3. Добавить **residual-connections**.   
Теперь сеть может обучаться крайне глубоко. Хоть на 1000 слоев - сеть все равно обучается.