>Хорошим примером ансамблей считается теорема Кондорсе «о жюри присяжных» (1784). Если каждый член жюри присяжных имеет независимое мнение, и если вероятность правильного решения члена жюри больше 0.5, то тогда вероятность правильного решения присяжных в целом возрастает с увеличением количества членов жюри и стремится к единице. Если же вероятность быть правым у каждого из членов жюри меньше 0.5, то вероятность принятия правильного решения присяжными в целом монотонно уменьшается и стремится к нулю с увеличением количества присяжных.  
  
# Бэггинг (от Bootstrap Aggregation)  
Выборка $X$ разделяется на $M$ подвыборок путём **бутстрэпа** (каждый элемент берётся из последовательности с его возвращением, т.е. возможны повторные значения). Для каждой из $M$ выборок строится классификатор $a_i(x)$. Итоговый классификатор усредняет ответы всех частных алгоритмов  
$$a(x)=\frac1M\sum^M_{i=1}a_i(x)$$  
Бэггинг позволяет снизить отличие истинного значения от предсказанного и предотвращает переобучение  
Метод применяется на маленьких выборках, потому что в больших выборках кореллированность ошибок будет выполняться, а при этом уменьшение ошибки будет незначительным  
  
### Случайный лес  
При использовании случайных лесов нет потребности в кроссвалидации или отдельном тестовом наборе.  
Алгоритм:   
- Берётся выборка $X$, разбивается на $N$ подвыборок с помощью бутстрэпа  
-  при каждом разбиении сначала выбирается $m$ случайных признаков из $n$ исходных, и оптимальное разделение выборки ищется только среди них.  
- Подобрать параметры для всех деревьев, такие как количество деревьев, максимальная глубина, минимальное количество сэмплов в сплите, мин. количество сэмплов в листе, критерий и т.д.  
- Построить дерево решений $b_n$ для каждой подвыборки для последующего мажоритарного голосования всех моделей с учётом вероятностей  
