Анализ семантики естественных языков, основанный на дистрибутивной семантике, машинном обучении и векторном представлении слов (**embedding**).  
2 архитектуры обучения: *CBoW* (Continuous Bag of Words) и *Skip-gram*  
  
### CBoW  
Предсказывает слово, исходя из окружающего контекта. Например, пару последний слов или по 2 слова по бокам.  
### Skip-gram  
Наоборот, исхода из текущего слова предсказывает окружение.   
  
#### **Тренировка и архитектура**:   
на примере с skip-gram, с помощью скользящего окна, для каждого слова определяется набор из 4 (гиперпараметр) слов, окружающих его. Текущее слово выступает как входное (input), а другое как метка (label).  
![Pasted image 20230304225447.png|500](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230304225447.png?raw=true)  
Слово fox подаётся в нейронку в виде вектора размером равным мощности словаря: *"0 0 0 . . . 1 . . . 0 0 0"*, где единице соответствует позиция слова *fox*.   
Далее этот вектор прогоняется через обычный линейный слой без функции нелинейности *U*.   
Далее, предполагается, что в скрытом слое (по центру) хранятся ключевые семантические свойства. Типичен размер от 250 до 1000.  
Слой *V* это также линейный слой, который преобразует скрытый слой в вектор с размером равным мощности словаря.   
После функции *softmax* вытаскивается слово с максимальным весом.   
![Pasted image 20230304231808.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230304231808.png?raw=true)  
Функция ошибки обычная *cross entropy loss*.   
> Можно привести аналогию с энкодером и декодером. Матрица *U* шифрует слова, выделяя главные черты, а матрица *V* раскрывает все эти черты для интерполяции.  
  
#### Negative sample  
Словарь, конечно же, имеет крайне большую мощность, а значит матрицы весов *U*, *V* в классическом понимании должны иметь гигантские размеры. Такое ни в оперативку ни в GPU не поместится. С матрицей *U* решение проблемы простое - пусть в в обучении испоьзуется только та строка, которая отвечает за входное слово (на картинке выше). С матрицей *V* так просто не получится.  
Решено вместо функции *softmax* можно использовать функцию вероятности, выдающую бинарный ответ - 1, если слово находится в достаточном окружении входного слова, иначе 0.  
Далее в *обратном распространении ошибки* используется выходное слово, заведомо равное 1, а также *k* (обычно от 2 до 20, гиперпараметр) слов из матрицы, по которым ответ должен бы быть 0. Градиент раскидывается только по этим строкам весов.  
	*Есть некоторые детали в фукнции ошибки, а также в хитром подборе негативных слов по их частоте и т.д.*  
  
#### Арифметика свойств  
Можно с помощью некоторых манипуляций вычитать и суммировать слова, для получения других семантических смыслов. Пример:  
$$king - man + woman ≈ queen$$  
Предсказывается несколько слов, которые ближе всего находятся в пространстве рядом (подробнее - читать embedding). Для ответа выбирается самое ближнее слово, то есть с самой высокой вероятностью.  
  
#### Предложения  
Также можно получить вектор целого предложения, если дообучить предтренированную на словах модель. Можно выцеплять основные ключевые смыслы целого предложения, например, негативный или положительный обзор на фильм по целому предложению.  
$$v(sentence) = \sum_w P(word)v(word)$$