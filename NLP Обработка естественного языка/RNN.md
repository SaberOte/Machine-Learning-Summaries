# Recurral Neural Network  
Имеется задача, когда нужно, чтобы одному слову/нескольким словам на входе соответствовало одно слово/несколько слов на выходе. Для этого и нужны рекуррентные сети, которые могут запоминать некоторую информацию с предыдущего шага прохода через сеть, например, при поступлении текста, по одной букве.  
![Pasted image 20230412010647.png|600](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230412010647.png?raw=true)  
На картинке, над буквой *a* показан проход через сеть, где синий блок - это рекурентный слой, а зеленый - обычный полносвязный. Состояние *h* с предыдущего прохода участвует в этом проходе. Результат вычисления слоя передаётся на следующий раз. Таким образом, у сети появляется память, которая хранит результат вычисления предыдущего прохода.  
![Pasted image 20230412010846.png|700](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230412010846.png?raw=true)  
На картинке выше код. Важно в нем то, что есть **веса для h**, которые с *h* умножаются, а есть **веса для X**, которые умножаются с входом. Результат вычислений и хранится для дальнейших проходов.  
Кстати, `np.tanh` - это функция нелинейности. Здесь используют обычно функцию *гиперболического тангенса*, а не *ReLu* для того, чтобы на выходе получались и отрицательные и положительные значения.  
  
### Задача генерации текста  
Рекуррентные сетки можно использовать для генерации текста, так как мы можем сохранять **контекст** и передавать его дальше от буквы к букве.  
![Pasted image 20230412012950.png|300](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230412012950.png?raw=true)  
На картинке нужно отметить специальный токен `[BOS]` (*Begin Of Sentence*), который добавлен в словарь как начало предложения.   
![Pasted image 20230412013144.png|500](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230412013144.png?raw=true)  
На этой картинке уже представлен конкретный пример тренировки, побуквенно. Оказывается, такой метод вполне может генерировать целый текст.  
![Pasted image 20230412013656.png|500](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230412013656.png?raw=true)  
Результат выше. На 2000 итерации хотя бы на первый взгляд похоже на что то. Также имеет смысл, на чем обучается эта шайтан-машина. Если ее обучать, например, на текстах писателей, то и текст генерировать она будет в похожем стиле - с персонажами, сносками и т.д.  
  
Также эта модель может хранить и передавать в контексте смысловую нагрузку. Например, может быть нейрон, который отвечает за то, негативный отзыв будет дальше генерироваться либо позитивный.   
Обычно тренируют такую сеть кусками текста. Например, берут куски текста по 200 слов. Но в начало следующего куска все таки отправляют состояние из предыдущего, чтобы не начинать с нуля.  
  
## Long Short-term Memory (LSTM)  
Существует проблема в случае с использованием самой простой рекуррентной сети. Контекст все таки получается очень локальным и, смысл, например, от начала абзаца до конца уже не доходит. В случае с обучением с помощью *обратного распространения* связанная с этим ошибка просто убивается в запредельных значениях гиперболического тангенаса (такая проблема была у сигмоиды тоже). То есть обучать такую тему на долгий контекст не выйдет. Для этого придумали **LSTM**.   
![Pasted image 20230413043915.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230413043915.png?raw=true)  
На картинке можно увидеть теперь **2 контекста**: *c* и *h*. Первый выступает долгосрочным контекстом. Таковым его делает особая инициализация *гейтов*. Гейты (*gates*) обозначаются на картинке как знак сигмы, и каждый имеет разное предназначение.   
Первый слева (forget gate) всегда инициализируется единичкой (размерность гейта конечно же равна размеру контекста. и конечно же там не записана единичка. Коэффициенты подобраны так, что функция сигмоиды, которая есть в формуле, выдавала число, близкое к единице). Так *С* никак не меняется из-за умножения на 1. Выполняет функцию такую, что, при необходимости, можно *забыть* некоторые детали. Само собой, тогда в этом участке значение веса в гейте должно оказаться меньше единицы.   
Второй gate (input gate) уже имеет функцию прибавки к *C*. Тут уже два элемента - элемент с сигмоидой и элемент с гиперболическим тангенсом. Первый решает, какую сделать добавку (сигмоида выдает числа `[0,1]`), а второй - её мощность и знак (tanh `[-1,1]`).   
Третий и четвертый гейты решают, какие все таки отдавать контексты на выходе.  
![Pasted image 20230413050020.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230413050020.png?raw=true)  
Преимущество во время обучения заключается в том, что градиент по *C* распостряняется очень легко в математическом плане, образуя *магистраль* для обратного распространения. Нет функции нелинейности и функции крайне простые - умножение при получении градиента уходит пропорционально, а при сложении и вовсе пилится по пропорциональным частям между слогаемыми, за счет чего обучение происходит без проблем.  
Помимо LSTM есть также GRU. У LSTM есть много различных вариантов реализации архитектуры.  
  
## Bidirectional RNN  
В задачах генерации текста нужен контекст предложений **до** текущего элемента. А вот в задаче, например, перевода нужен конекст как и предыдущий, так и следующий. Для таких задач и сущесвуте двусторонняя рекуррентная сеть.  
![Pasted image 20230413051401.png](https://github.com/PolkaDott/Data-Science-Summaries/blob/main/NLP%20Обработка%20естественного%20языка/attachments/Pasted%20image%2020230413051401.png?raw=true)  
Здесь есть два набора весов для формирования *h*: для переднего и для заднего контекста. К примеру, берется предложение и каждое слово или буква прогоняются для создания прямого контекста. Затем обратно прогоняется для создания заднего контекста. И затем, после этого уже прогоняется и сама сеть. 